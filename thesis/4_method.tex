%! Author = ASUS
%! Date = 7/2/2023

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{lipsum}
\usepackage{wasysym}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}
% Document
\begin{document}
    We will explore the application of Structure from Motion in a real-world scenario within the field of 3D
    computer vision. Our goal is to localize a robot in a large city environment using point cloud registration.
    Localization refers to the task of determining the position and orientation of a robot within its environment.
    And, point cloud registration is defined as finding the transformation, i.e. translation and orientation,
    that aligns the points in one point cloud with those in another.

    Outdoor localization, today, relies heavily on GPS technology, accompanied by ground based augmentation systems
    to improve accuracy. However, in this thesis, we are going to explore a method that makes it offline, meaning
    eliminating the need for GPS or similar devices.

    \section{Point Cloud Generation}
    \paragraph{Aerial Point Cloud:} An extensive collection of 3D points is obtained from the city of Padova
    using an airplane. The covered region spans 1600m by 1000m, and the average nearest neighbor distance
    of points is 0.63m. The dataset contains a total of 3,583,803 points, figure \ref{fig:ply_aerial_all}.
    \paragraph{Ground Point Clouds:}
    A camera is mounted on the head of a vehicle, and videos are taken from the streets. Then, SfM algorithm is used to
    reconstruct the 3D sence of the environment. There are 2 types of reconstruction:
    \begin{itemize}
        \item Sparse Reconstruction estimates the positions of a relatively small number of keypoints detected on the input images
        \item Dense Reconstruction provides 3D positions for all pixels in the input images by generating depth maps.
    \end{itemize}

    Figure \ref{fig:sfm} shows examples of both reconstructions.
    For our method, we need dense point clouds.

    \begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/experiment/ply_aerial_all}
    \caption{The aerial point cloud from the city of Padova}
    \label{fig:ply_aerial_all}
    \end{figure}

    \begin{figure}
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/method/sfm_sparse_1}
            \caption{Example 1: Sparse Point cloud}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/method/sfm_dense_1}
            \caption{Example 1: Dense Point cloud}
        \end{subfigure}

        \vspace{1em}

        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/method/sfm_sparse_2}
            \caption{Example 2: Sparse Point cloud}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/method/sfm_dense_2}
            \caption{Example 2: Dense Point cloud}
        \end{subfigure}

        \caption{Comparing sparse and dense reconstructions, generated by COLMAP ~\cite{schoenberger2016sfm} and ~\cite{schoenberger2016mvs}}
        \label{fig:sfm}
    \end{figure}


    \section{Point Cloud Registration}
    The objective is to align the ground point cloud with the aerial point cloud. Initially, we attempted
    to register the point cloud using traditional global and local algorithms like FPFH feature matching,
    RANSAC, and ICP. However, due to the significant differences in the nature of the datasets, the results were
    too poor. The aerial point cloud primarily consists of street points and building roofs, as it is captured
    from above. While, the ground point cloud contains street points and building walls. Therefore, we had to
    find the common features and simplify the problem. The new pipeline could be described as follows:
    \begin{enumerate}
        \item It was observed that viewing both point clouds from top point of view, aligned with the z-axis, provides
        valuable information. The first step is to align the point clouds with the z-axis. The aerial
        point cloud is already aligned. The ground point cloud is segmented into planes, and the plane
        with the highest point count is identified as the street plane, since it is the common plane among
        all streets and directions and so more points must fall on the ground plane. After that,
        the rotation matrix between the normal vector of the street plane and the (x=0, y=0, z=1) vector
        is computed, and then, the ground point cloud is rotated using the rotation matrix.
        This step determines the rotation around x and y axes, roughly.

        \item From the top point of view, It is seen that the most discriminative feature among the datasets
        is the angles of streets and crossroads. So, the data of vertical walls in ground dataset and roofs
        in aerial dataset has no use. Therefore, both point clouds are sliced along the z-axis with a certain
        threshold, and the half related to the streets is remained. In figure \ref{fig:ply_sliced}, the similarity of
        both sliced ground and aerial point clouds are visible.
        \begin{figure}
            \centering
            \begin{subfigure}{0.45\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/experiment/ply_sliced_ground}
                \caption{Ground Point Cloud}
            \end{subfigure}
            \hfill
            \begin{subfigure}{0.45\textwidth}
                \centering
                \includegraphics[width=\linewidth]{images/experiment/ply_sliced_aerial}
                \caption{Aerial Point Cloud}
            \end{subfigure}
            \caption{Point clouds sliced from the top viewpoint}
            \label{fig:ply_sliced}
        \end{figure}

        \item Since the generated point clouds has the up-to-scale problem, it is needed to normalize the
        coordinate values across all datasets. This step does not scale the ground point clouds to the actual size
        in aerial point cloud. However, it ensures that the average minimum distance between the points is the same
        among all of the generated point cluds.
        Each point cloud is scaled by the factor of 1 divided by the current average of the minimum distance to the nearest point.

        \item A 2D binary map is generated for each point cloud considering only x and y coordinates of all 3D points.
        The dimensions of the map are determined by calculating the difference between the maximum and minimum x and y
        coordinates of all points and dividing it by a predefined resolution value. Each cell in the map is assigned
        a value of 1 if at least one point's x and y coordinates fall within that cell, indicating the presence of
        points in that area. Conversely, if there are no points corresponding to the respective x and y coordinates,
        the cell is marked as 0. In the binary grid map for Ground Point Cloud, considering that each point cloud is
        generated from a sequence of video frames, the camera pose of the middle frame is regarded as the ground truth
        center position for the entire dataset. To evaluate the registration method's robustness and generalization,
        3 aerial point clouds are considered for each ground dataset. These aerial point clouds share the same center
        as the aforementioned ground point cloud, but they differ in distance from the center. They are categorized
        as "easy," "medium," and "hard" with distances of 100, 150, and 200 meters from each direction, respectively.
        The binary grid map is generated for these aerial point clouds using the same logic as the binary grid map
        created for the ground point clouds.

        \item To localize the ground grid map within the aerial grid maps, various methods were explored, including
        2D feature matching, crossroad detection based on point counting, and training convolutional neural networks,
        etc. Among these approaches, template matching has the best results. The template matching process involves
        comparing a small template image, which represents the desired pattern (in this case, the ground grid map),
        with different regions of the target image (the aerial grid map), and identify the region that has the highest
        similarity with the template. This is achieved by moving a window across the target
        image and comparing the template with each window. We extended the algorithm to compare also different
        scales and rotations of the template image. The template is rotated upto 360 degrees with an interval of 10 degrees and
        scaled from 10\% to 200\% of its initial size, increasing 10 percent per each attempt.

    \end{enumerate}

    The whole pipeline can be summarized as follows:
    \begin{enumerate}
        \item Take the video of the streets, and extract the frames
        \item Run sparse SfM algorithm
        \item Refine the sparse point cloud and camera poses using Pixel Perfect algorithm
        \item Generate a dense point cloud from the refined data
        \item Align the point cloud along the z-axis.
        \item Scale and Slice the ground point cloud, and keep the points with z coordinate less than a threshold,
        in our case, the threshold is 20\% of the minimum z-coordinate
        \item Generate the binary grid map
        \item Slice Aerial point cloud along the z-axis with certain threshold, in our case, the threshold is 20\%
        of the minimum of z coordinate of all points
        \item Generate Binary grid map for Aerial point cloud
        \item Run Template Matching algorithm to localize the ground grid map inside the aerial grind map
    \end{enumerate}


\end{document}